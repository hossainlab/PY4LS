---
title: "CONJ620: CM 2.3"
subtitle: "Coding Style + Linear Models"
author: "Alison Hill"
output:
  xaringan::moon_reader:
    css: ["default", "css/ohsu.css", "css/ohsu-fonts.css"]
    header-includes:
    - \usepackage[amsmath]{amsmath}
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(moderndive)
library(extrafont)
library(broom)
```

```{r data, include = FALSE}
crimenames <- c("county", "region_name", "region_code",
               "criminals", "public_houses", "school_attendance",
               "worship_attendance")

crime <- read_table(here::here("data", "beerhall.dat"),
                    col_names = crimenames)
# For ggplot2
ggplot2::theme_set(ggplot2::theme_minimal() +
                   ggplot2::theme(text = element_text(family = "Lato")))
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warn = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 3, fig.width = 5.5, dpi = 300,
                      warning = FALSE, message = FALSE) 
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
```

class: center, middle

# Coding style

---

## Style guide

>"Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread."
>
>Hadley Wickham

- Style guide for this course is based on the Tidyverse style guide: http://style.tidyverse.org/

- There's more to it than what we'll cover today

---

## File names and code chunk labels

- Do not use spaces in file names, use `-` or `_` to separate words
- Use all lowercase letters

```{r eval = FALSE}
# Good
ucb-admit.csv

# Bad
UCB Admit.csv
```

---

## Object names

- Use `_` to separate words in object names
- Use informative but short object names
- Do not reuse object names within an analysis

```{r eval = FALSE}
# Good
acs_employed

# Bad
acs.employed
acs2
acs_subset
acs_subsetted_for_males
```

---

## Spacing

- Put a space before and after all infix operators (=, +, -, <-, etc.), and when naming arguments in function calls. 
- Always put a space after a comma, and never before (just like in regular English).

```{r eval = FALSE}
# Good
average <- mean(feet / 12 + inches, na.rm = TRUE)

# Bad
average<-mean(feet/12+inches,na.rm=TRUE)
```

---

## ggplot

- Always end a line with `+`
- Always indent the next line

```{r eval = FALSE}
# Good
ggplot(diamonds, mapping = aes(x = price)) +
  geom_histogram()

# Bad
ggplot(diamonds,mapping=aes(x=price))+geom_histogram()
```

---

## Long lines

- Limit your code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font.

- Take advantage of RStudio editor's auto formatting for indentation at line breaks.

---

## Assignment

- Use `<-` not `=`

```{r eval = FALSE}
# Good
x <- 2

# Bad
x = 2
```

---

## Quotes

Use `"`, not `'`, for quoting text. The only exception is when the text already contains double quotes and no single quotes.

```{r eval = FALSE}
ggplot(diamonds, mapping = aes(x = price)) +
  geom_histogram() +
  # Good
  labs(title = "`Shine bright like a diamond`",
  # Good
       x = "Diamond prices",
  # Bad
       y = 'Frequency')
```

---
class: inverse, middle, center
## Back to linear models

---

## Vocabulary

- **Outcome variable:** Observed variable whose behavior or variation you are trying to understand, on the y-axis (aka dependent or response variable)

--

- **Explanatory variables:** Other observed variables that you want to use to explain the variation in the outcome, on the x-axis (aka independent or predictor variables)

--

- **Fitted values:** Output of the **model function** (aka predicted values)
    - The model function gives the typical value of the response variable
    *conditioning* on the explanatory variables

--

- **Residuals:** Show how far each case is from its fitted model value
    - Residual = Observed outcome value - Fitted value
    - Tells how far above/below the model function each case is

---
## The conceptual linear model

[ModernDive](http://moderndive.netlify.com/6-regression.html#model1table) presented us with this formula:

$$\widehat{y} = b_0 + b_1 \cdot x$$

Where:

* the intercept coefficient is $b_0$, or the value of $\widehat{y}$ when $x=0$, and 
* the slope coefficient $b_1$, or the increase in $\widehat{y}$ for every increase of one in $x$.

---
class: center, middle
## Let's break down this formula

$$\widehat{y} = b_0 + b_1 \cdot x$$

<br>
--

$$\downarrow$$
<br>
--

$$\overbrace{\widehat{y}}^{fitted} = \overbrace{b_0}^{intercept} + \overbrace{b_1}^{slope} \cdot x$$

---
class: center, middle
## The linear model in R

$$\widehat{y} = b_0 + b_1 \cdot x$$
<br>
--

$$\downarrow$$
<br>
--

$$y = \overbrace{b_0 + b_1 \cdot x}^{\hat{y}} + e$$
<br>
--

$$\downarrow$$

<br>
--

```{r eval = FALSE}
lm(y ~ x, data = dataframe) 
```

---
## `lm` in R

```{r}
(crime_from_ph <- lm(criminals ~ public_houses, data = crime))
```


--

<br>

$$\widehat{\textrm{criminals}} = 109.34 + .12~\textrm{public_houses}$$
--

- **Slope:** For each additional public house per 100k people, the crime per 100k people is expected to be higher, on average, by 0.12.

--

- **Intercept:** Counties with 0 public houses are expected to have crime rates at 109.34 per 100k people, on average.
    - Does this make sense?

---
## Interpreting `lm` in R


```{r basic_lm, echo=FALSE, out.width='70%'}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", colour = "dodgerblue", se = FALSE) 
```


$$\widehat{\textrm{criminals}} = 109.34 + .12~\textrm{public_houses}$$


- **Slope:** For each additional public house per 100k people, the crime per 100k people is expected to be higher, on average, by 0.12.



- **Intercept:** Counties with 0 public houses are expected to have crime rates at 109.34 per 100k people, on average.
    - Does this make sense?
    
---
## Intercept

- **Intercept:** Counties with 0 public houses are expected to have crime rates at 109.34 per 100k people, on average.
    - Does this make sense? 
    - Do we have any counties with 0?
    
```{r}
crime %>% 
  filter(public_houses == 0)
```

While the intercept has a mathematical interpretation (it situates the vertical location of the line), it may not always have a practical one.

--

A solution we'll talk more about is ["centering" your variables](https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00012.x) before using `lm`.

---

## Properties of the least squares regression line

- The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$: $(\bar{x}, \bar{y})$:

$$\hat{y} = b_0 + b_1 x ~ \rightarrow ~ b_0 = \hat{y} - b_1 x$$

- The slope has the same sign as the correlation coefficient:

$$b_1 = r \frac{s_y}{s_x}$$

- The sum of the residuals is zero: 
$$\sum_{i = 1}^n e_i = 0$$

- The residuals and $x$ values are uncorrelated.
---
## Fitted and residual values

```{r}
(crime_pts <- get_regression_points(crime_from_ph))
```

---
## Fitted values

What do you see?

```{r echo=FALSE}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", colour = "dodgerblue", se = FALSE) + 
  coord_cartesian(xlim = c(0, 1000)) +
  scale_x_continuous(expand=c(0,0), limits=c(0,1000))
```


---

## Fitted values

Caution: extrapolating...

```{r echo=FALSE}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", colour = "dodgerblue", se = FALSE, 
              fullrange = TRUE) + 
  coord_cartesian(xlim = c(0, 1000)) +
  scale_x_continuous(expand=c(0,0), limits=c(0,1000))
```

---
class: center, middle
## Break the formula down once more...


$$y = b_0 + b_1 \cdot x + e$$
<br>
--

$$\downarrow$$
<br>
--

$$\overbrace{y}^{observed} = \overbrace{b_0 + b_1 \cdot x}^{fitted} + \overbrace{e}^{residual}$$

<br>
--

$$\downarrow$$

<br>
--

$$\overbrace{y}^{outcome} = \overbrace{b_0 + b_1 \cdot x}^{model} + \overbrace{e}^{error}$$

---
class: center, middle
## All of statistics


$$\overbrace{y}^{outcome} = \overbrace{b_0 + b_1 \cdot x}^{model} + \overbrace{e}^{error}$$

<br>
--

$$\downarrow$$

<br>
--

$$\textrm{outcome}_i = \textrm{(Model}_i) + \textrm{error}_i$$
<br>
--

$$\downarrow$$

<br>
--

> *"All models are wrong but some are useful"* -George Box

---
class: middle, inverse, center

## So, how useful is your simple linear regression model?

--

1. Create a scatterplot with the residuals on the $y$-axis and the original explanatory variable $x$ on the $x$-axis.

--

2. Create a histogram of the residuals, thereby showing the *distribution* of the residuals.

--

## What are we looking for?

---

## Variation around the model...

is just as important as the model, if not more!

*Statistics is the explanation of variation in the context of what remains
unexplained.*

- The scatter suggests that there might be other factors that account for large parts 
of country-to-county variability, or perhaps just that randomness plays a big role.

- Adding more explanatory variables to a model can sometimes usefully reduce
the size of the scatter around the model. (We'll talk more about this later.)

---
## Scatterplot of the residuals<sup>1</sup>

"1. the residuals should be on average 0. In other words, sometimes the regression model will make a positive error in that $y - \widehat{y} > 0$, sometimes the regression model will make a negative error in that $y - \widehat{y} < 0$, but *on average* the error is 0." 

```{r echo = FALSE, out.width='75%'}
ggplot(crime_pts, aes(x = public_houses, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, lwd = 3, color = "turquoise") + 
  geom_hline(aes(yintercept = mean(residual)), 
             color = "dodgerblue")
```



.footnote[
[1] From [ModernDive](http://moderndive.netlify.com/6-regression.html#model1residuals)

]


---
## Scatterplot of the residuals<sup>1</sup>

"2. Further, the value and spread of the residuals should not depend on the value of $x$."

```{r echo = FALSE, out.width='75%'}
ggplot(crime_pts, aes(x = public_houses, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, lwd = 3, color = "turquoise") + 
  geom_smooth(color = "dodgerblue")
```



.footnote[
[1] From [ModernDive](http://moderndive.netlify.com/6-regression.html#model1residuals)
]

---
## Histogram of the residuals<sup>1</sup>

"we would like the residuals to be normally distributed with mean 0. In other words, be bell-shaped and centered at 0!"

```{r, out.width='75%'}
ggplot(crime_pts, aes(x = residual)) +
  geom_histogram(binwidth = 18, color = "white") 
```

.footnote[
[1] From [ModernDive](http://moderndive.netlify.com/6-regression.html#model1residuals)
]

---
class: center, middle
## All of statistics


$$\overbrace{y}^{outcome} = \overbrace{b_0 + b_1 \cdot x}^{model} + \overbrace{e}^{error}$$

<br>
--

$$\downarrow$$

<br>
--

$$\textrm{outcome}_i = \textrm{(Model}_i) + \textrm{error}_i ~ \rightarrow ~ \textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$
<br>
--

$$\downarrow$$

<br>
--

> *"All models are wrong but some are useful"* -George Box

---
class: middle, inverse, center


## So, how useful is your simple linear regression model?

We are now going to answer this a different way...


---
## Residuals

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"?
1. What is the "model"?

```{r fitted_model, echo = FALSE}
ggplot(crime, aes(public_houses, criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(aes(x = public_houses, 
                 y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21)  
```



---
## Residuals

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"? $y_i$ (dark blue dots)
1. What is the "model"? $\hat{y_i}$ (bright blue dots)

![](`r knitr::fig_chunk('fitted_model','png')`)



---
## What is the "model" saying?

"The bright blue points are what we would expect to see if there was `___` a `___` of `r round(cor(crime$criminals, crime$public_houses), 2)` between `public_houses` and `criminals`"

![](`r knitr::fig_chunk('fitted_model','png')`)

---
## Sums of Squares Residual

The total amount of error present when the best-fitting linear model is applied to the data. 

$$SS_{residual} = \sum{e_i^2} = \sum{(y_i - \hat{y_i})^2}$$
--

```{r ss_resid, echo = FALSE, out.width='75%'}
ss_resid <- ggplot(crime, aes(public_houses, criminals)) +
  geom_segment(aes(x = public_houses, 
                   xend = public_houses, 
                   y = criminals, 
                   yend = predict(lm(criminals ~ public_houses))), 
               colour = "red") +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(aes(x = public_houses, 
                 y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21)  
ss_resid
```


---
## Least squares regression

The regression line minimizes the sum of squared residuals.

$$\hat{y} = b_0 + b_1~x$$
What is another (equivalent) way to say the same thing?

--

$$y_i = b_0 + b_1~x_i + e_i$$

$$y_i = \hat{y_i} + e_i$$

--

If $e_i = y_i - \hat{y_i}$,

then, the regression line minimizes $SS_{residual} = \sum_{i = 1}^n e_i^2$.

---
class:middle, inverse, center

## The `SS Residual` compares the best-fitting linear model to the *true (unknown) data-generating model*

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

$$SS_{residual} = \sum{e_i^2} = \sum{(y_i - \hat{y_i})^2}$$
---

## Can you spot another model?

```{r echo = FALSE}
crime_scatter <- ggplot(crime, aes(public_houses, criminals)) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(size = 2, color = "navy", alpha = .8) 
crime_scatter
```

---
## Hint

I ask you to guess how many criminals are in county X, but I don't tell you how many public houses there are.

![](`r knitr::fig_chunk('basic_lm','png')`)
---

## The mean as a model

```{r echo = FALSE}
crime_scatter +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") 
```
---
## The mean as the "null" model

"The turquoise points are what we would expect to see if there was `___` association between `public_houses` and `criminals`"

```{r mean_model, echo = FALSE}
crime_scatter +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) 
```


---
## If the mean is the model...

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"? 
1. What is the "model"? 

--

![](`r knitr::fig_chunk('mean_model','png')`)

---
## If the mean is the model...

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"? $y_i$ (dark blue dots)
1. What is the "model"? $\bar{y}$ (turquoise dots)

--

![](`r knitr::fig_chunk('mean_model','png')`)

---
## What is this "model" saying?

Ignore the regression line/predicted points

What would the magnitude of the differences between the observed `criminals` values and the mean value tell us? *(remember what we said the mean line represented: `___` correlation between variables)*

```{r ss_total, echo = FALSE}
ss_total <- ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue", alpha = .2) +  
  geom_segment(aes(x = public_houses, 
                   xend = public_houses, 
                   y = criminals, 
                   yend = mean(criminals)), 
               colour = "orchid") +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
ss_total
```
---
## What about spread?

Variance is calculated by taking the sum of squared differences from the mean, and dividing by `N-1`


```{r echo = FALSE}
crime %>% 
  summarize(var_crime = var(criminals),
            sd_crime = sd(criminals))
```

--


```{r}
crime %>% 
  summarize(sst_crime = sum((criminals - mean(criminals))^2),
            var_crime = sst_crime / (n() - 1),
            sd_crime = sqrt(var_crime))
```


---
## Take another look

The sum of squared differences between the `___` and the `___` values of `criminals` is the numerator of the variance (said another way, divide by `N-1` to get `var(criminals)`)

--

```{r echo = FALSE}
ss_total
```


---

## Sums of Squares Total

The __"Sums of Squares Total"__ is the total amount of error present when the most basic model is applied to the data. 

--

What is the "most basic model"? The mean (aka null) model!

--

$$SS_{total} = \sum{(y_i - \bar{y})^2}$$

--

![](`r knitr::fig_chunk('ss_total','png')`)



---
## A thought experiment...3 "models"?

1. Zero association between the variables (aka, the mean / null model)
2. An exact association of `r round(cor(crime$criminals, crime$public_houses), 2)` between the variables
3. The true (but always unknown) data-generating model

```{r echo = FALSE}
ggplot(crime, aes(public_houses, criminals)) +
  geom_vline(aes(xintercept = min(public_houses)),
             lwd = 5, color = "yellow") +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  geom_point(aes(x = public_houses, 
             y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
```


---
## A thought experiment...

Are there any other "differences" we can calculate with this set of 3 points/models?

```{r echo = FALSE}
crime_scatter +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  geom_point(aes(x = public_houses, 
             y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
```

---
## A thought experiment...(last!)

What about the difference between the mean and predicted values?

```{r echo = FALSE}
ss_model <- crime_scatter +
  geom_segment(aes(x = public_houses, 
                   xend = public_houses, 
                   y = predict(lm(criminals ~ public_houses)), 
                   yend = mean(criminals)), 
               colour = "turquoise") +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  geom_point(aes(x = public_houses, 
             y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
ss_model
```

---
## A thought experiment...(last!)

The __"Sums of Squares Model"__ is the total amount of error present when the fitted values from the best-fitting linear model are compared to the fitted values from the most basic model.

```{r echo = FALSE}
ss_model
```

---
## All together now

.pull-left[
$$SS_{total} = SS_{model} + SS_{residual}$$

<br>

$$SS_{total}$$
```{r echo = FALSE}
ss_total
```

]

.pull-right[
$$SS_{model}$$
```{r echo = FALSE}
ss_model
```

$$SS_{residual}$$
```{r echo = FALSE}
ss_resid
```
]



---
## Summing the sums of squares

$$SS_{total} = SS_{model} + SS_{residual}$$


total variation = "explained" variation + residual ("left over") variation

---
## All of statistics

$$ \textrm{outcome}_i = \textrm{(Model}_i) + \textrm{error}_i$$

$$SS_{total} = SS_{model} + SS_{residual}$$

$$R^2 = SS_{model} / SS_{total}$$

--

also true...
$$R^2 = (1 - SS_{residual}) / SS_{total}$$
```{r}
library(broom)
glance(crime_from_ph)
```

